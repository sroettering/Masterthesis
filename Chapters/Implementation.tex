%************************************************
\chapter{Implementation}\label{ch:implementation}
%************************************************

\begin{itemize}

\item{
	Intro text describing what is implemented, features, hardware
}
\item{
	Show user interface and describe mechanics. how to speech and gesture. mention activation angle study and show plot. 
}
\item{
	Mention Modules and communicationManager. show communication protocol. maybe uml diagram of whole application
}
\item{
	Describe Handheld module. introduce spotify api + webapi wrapper.
}
\item{
	Describe Wear module
}
\item{
	possible improvements and restrictions to the program (like leaving out unneeded spotify features), input feedback
}

\end{itemize}

This chapter describes the smartwatch controlled mobile music player implementation. It gives an insight into the chosen interaction techniques, the mobile-wear communication and the core music player functions.

The essential music library is provided by a private Spotify\footnote{Music streaming service: \url{www.spotify.com}} premium account. The prototype is divided into two separate applications, i.e. mobile (handheld) and wear (smartwatch). Users are able to access all features of the mobile application via the smartwatch application. In order to make use of the focused-casual continuum \cite{pohl2013focused}, three interaction techniques, differing in the amount of control granted, are available for the user:
\begin{itemize}
\item{Touch}
\item{Speech}
\item{Gesture}
\end{itemize}
Each technique can be used for the simpler actions of the music player such as play and pause, skip to previous or next song and changing the volume. However, for the more complex interactions, e.g. choosing a playlist, only the touch and speech interaction methods suffice. Gesture and speech input can be performed casually while not even looking at the device (the smartwatch). The following section describes the functionality and the power of the different methods.

Both applications are implemented for the android platform. The handheld device has no further hardware or software requirements other than supporting the android wearable \ac{API} so a Samsung Galaxy S6 is chosen. The smartwatch application certainly requires the device to be equipped with a acceleration sensor and a microphone. A moto360 from Motorola is well suited for this. Both devices need to support Bluetooth, too.

\section{User Interface}\label{sec:UserInterface}
The \ac{GUI} from the mobile application only differs from the wear \ac{GUI} in terms of appearance. In terms of control over the music player, both versions offer the same possibilities and show the same information (when connected) at any given time. The wear application, however, additionally introduces speech and gesture interactions which the mobile application is not capable of.

It is noteworthy that the Spotify \ac{SDK} offers a lot more functionality than implemented, such as creating own playlists or searching the entire Spotify music library for keywords. The implemented music player, however, just offers the basic playback functionalities in order to keep the application simple and to not overload the experiment participants with information since they have to remember how to control the music player.

\subsection{Touch Input - \ac{GUI}}
Both \ac{GUI}s are designed to be simple and straightforward. The mobile version mainly consists of two areas. Figure \ref{fig:mobileGUI} depicts a screenshot of this layout. A control panel is situated at the bottom of the screen. Above this is a left-right scrollable pager containing different kinds of lists. \marginpar{Categories are Spotify's extended version of genres} The scrollable list pager contains five lists, one for each music arrangement which are playlists, songs, albums, artists and categories.
\begin{figure}[bth]
	\myfloatalign
	\includegraphics[width=.45\linewidth]{img/mobileGUIlabeled.png}
	\caption{\ac{GUI} of the mobile application. (1) shows the left-right scrollable list pager with indicators which list is shown at the top. (2) shows the player control panel in blue.}
	\label{fig:mobileGUI}
\end{figure}
The control panel located beneath the list pager contains four control buttons such as a shuffle button including on-off indicator, a skip to previous song button, a play-pause button and a skip to next song button. Unlike the wear application, the mobile version spares buttons for controlling the volume, because most devices own hardware buttons for this purpose. Information on the name of the current song and artist can be found above these buttons. The track's progress and total duration can be found at the very bottom of the screen. \\

However, the intention of the wear application is, that the user does not need to bother reaching his mobile phone. For this reason, the wear application's \ac{GUI} offers the same amount of touch control over the music player. Figure \ref{fig:wearGUI} demonstrates the layout of the wear application. It consists of a GridViewPager with three horizontal pages. 

\begin{figure}[bth]
	\myfloatalign
	\subfloat[control panel in interactive mode]
	{\label{fig:wearGUI-controls}
	\includegraphics[width=.3\linewidth]{img/wearGUIcontrolsPage.png}} \quad
	\subfloat[list of music arrangements]
	{\label{fig:wearGUI-select}
	\includegraphics[width=.3\linewidth]{img/wearGUIselect.png}} \quad
	\subfloat[list of playlists]
	{\label{fig:wearGUI-items}
	\includegraphics[width=.3\linewidth]{img/wearGUIitems.png}} \\
	\subfloat[control panel in ambient mode]
	{\label{fig:wearGUI-ambient}
	\includegraphics[width=.3\linewidth]{img/wearGUIambient.png}}
	\caption{ The \ac{GUI} of the wear application consists of a GridViewPager containing three horizontal pages. A dot indicator at the top of the screen shows the current page. \ref{fig:wearGUI-ambient} depicts the controls page in ambient mode, i.e. battery saving mode, while \ref{fig:wearGUI-controls} is the same screen in interactive mode. Swiping right brings up the list of music arrangements \ref{fig:wearGUI-select}. Selecting e.g. playlists switches to the third page \ref{fig:wearGUI-items} showing a list with all playlists.}
	\label{fig:wearGUI}
\end{figure}

Figure \ref{fig:wearGUI-controls} shows the main control page which is divided into five rows (1) - (5). A dot indicator showing the current page with a bigger dot is located at the top of the screen in row (1). Row (2) contains the current artist and track that is playing. Row (3) and (4) contain the control buttons, i.e. in (3) the buttons for play previous song, toggle play and pause, play next song and in (4) the buttons for decrease volume, toggle shuffle (grey = off, green = on) and increase volume are located. Row (5) contains two icons which indicate whether speech and gesture recognition are active (green) or inactive (grey).

Figure \ref{fig:wearGUI-select} shows the page in the middle which is reached by swiping over the screen from right to left. The page contains a vertical scrollable list of the five music arrangements, namely playlists, songs, albums, artists and categories. Selecting a list entry scrolls to the right (the third) page which always adapts its list showing the respective items in the selected arrangement. \\

The moto360 smartwatch is equipped with a battery saving mode called ambient mode. In this mode cpu processing is reduced and it is recommended\footnote{\url{https://developer.android.com/training/wearables/apps/always-on.html}} to reduce the \ac{UI} layout to a black background and white text color. Updates to the \ac{UI} should then happen on a several seconds up to a minute basis. Applications that run in both ambient and interactive mode are called \textit{always-on apps}. The transition from interactive to ambient mode happens either automatically after a short period of user inactivity or can be forced by covering the screen. Leaving ambient mode happens either by touching the screen or by bringing up the wrist as in looking at the time. Figure \ref{fig:wearGUI-ambient} depicts the control page of the wear application running in ambient mode with a black background and white text and icon color. \\

\subsection{Speech Control}
The wear application offers the possibility to control the music player via speech commands. In order to enter a speech command, the application must be in interactive mode which activates the continuous speech recognition (indicated by the green microphone seen in \ref{fig:wearGUI-controls} at the bottom of the screen). The user can then talk to the watch and issue a command. Speech data is recorded and converted to text by the android speech recognition service\footnote{\url{https://developer.android.com/reference/android/speech/SpeechRecognizer.html}}. Since the speech recognition happens continuously input must be recognized easily as a possible valid command. Therefor every command has to end with a special final keyword, the word \textit{``bitte''} \marginpar{``Bitte'' is the german word for ``please''}. As soon as the wear application detects this keyword, the converted text is send to the mobile application where it gets parsed, transformed into a command and finally executed. Figure \ref{fig:speechActivityDiagram} illustrates this procedure with an activity diagram. The wear-mobile communication process is described in more detail in section \ref{sec:Communication}.
\begin{figure}[bth]
	\myfloatalign
	\includegraphics[width=.99\linewidth]{img/SpeechActivityDiagram.png}
	\caption{Activity diagram of the speech interaction procedure. Actions to the left of the dashed line happen in the wear application and actions to the right of the dashed line happen in the mobile application. At stage (1) a preliminary decision is made whether the converted text resembles a command or not.}
	\label{fig:speechActivityDiagram}
\end{figure}

Speech commands for this music player can be divided into three groups based on their complexity. The first group contains the simplest commands that just consist of one keyword. Commands in the second group start with a keyword and are followd by additional words, until the final keyword occurs, that are handled as the input parameters. Commands in the third group are built by chaining different keywords together forming a simple sentence.

Table \ref{tab:speechCommands} contains the respective keywords for commands in the first two groups. The first six simple keywords each represent a command by itself and thus belong to group 1. The \textit{play} command is the only command in group 2 meaning it requires a parameter to work as intended.
\newpage
\begin{table}[Hbt]
	\myfloatalign
	\begin{tabularx}{\textwidth}{XX} \toprule
		\tableheadline{Speech Command Keywords} & \tableheadline{Actions in Music Player} \\ 
		\midrule
		pause & pause music playback \\
		weiter & resume music playback \\
		n\"achstes & skips to next song \\
		zur\"uck & skips to previous song \\
		lauter & increases the volume \\
		leiser & decreases the volume \\
		\midrule
		play <music element> & load and play an element from the music library \\
		\bottomrule
	\end{tabularx}
	\caption{Speech commands from the first two groups. The play command is the only command accepting parameters. All other commands in this table belong to group 1. To issue a command, simply activate speech recognition by entering the watches interactive mode and say the keyword followed by the final keyword ``bitte''.}
	\label{tab:speechCommands}
\end{table}

Commands in the third group, which are built by chaining the keywords shown in table \ref{tab:speechCommandsAudioFeature} together, offer a special kind of control over the music player. They allow for queuing and playing tracks that differ in terms of an audio feature from the current playing track.
\begin{description}
	\item[Audio Feature:] Spotify runs a suite of audio analysis algorithms on every track in their catalog. These extract about a dozen high-level acoustic attributes from the audio. Some are well-known musical features, like tempo and loudness. Others are more specialized, like valence and energy.
\end{description}

The four most intuitive audio features are supported by the music player to be used with speech commands, which are\footnote{A complete list of audio features and their meaning can be found at: \url{https://developer.spotify.com/web-api/get-several-audio-features/}}:
\begin{description}

	\item[Tempo:]The estimated tempo of a track measured in \ac{BPM}. It derives directly from the average beat duration.
	\item[Energy:]A measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale.
	\item[Loudness:]The overall loudness of a track in decibles (dB). Loudness values are averaged across the entire track. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values range between -60 and 0 db.
	\item[Valence:]A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive, while tracks with low valence sound more negative.
\end{description}

\begin{table}[h]
	\myfloatalign
	\begin{tabularx}{\textwidth}{XXX} \toprule
		\tableheadline{Scaling} & \tableheadline{Direction} & \tableheadline{Audio Feature} \\ 
		\midrule
		etwas & mehr & Tempo \\
		viel & weniger & Energie \\
		 & & Lautst\"arke \\
		 & & Stimmung \\
		\bottomrule
	\end{tabularx}
	\caption{Building blocks (keywords) for commands from the third group. Choose a keyword from each column, chain them to a sentence and attach the final keyword ``bitte'' at the end. The scaling keyword can be omitted.}
	\label{tab:speechCommandsAudioFeature}
\end{table}

Changing tracks based on the current track's audio features enables the user to specify what kind of music she wants to listen to in a situation, without deciding on the exact tracks to be played. The pseudo code in listing \ref{lst:audioFeature} is responsible for searching the corresponding tracks in the music library. The new value for the specified audio feature is calculated by first evaluating the direction keyword which determines whether to increase or decrease the current value. The scale keyword then determines the scaling factor which indicates by how much the current value is changed. Therefor the scaling factor is multiplied by the distance between the current value and either the global minimum or maximum value for the given audio feature depending on the direction keyword. Afterwards, tracks in  range of the new calculated value, determined by a tolerance radius, are searched. The tolerance radius is increased by a fix amount as long as no tracks are found. Figure \ref{fig:audioFeatureExample} explains this mechanism with example values.

\begin{lstlisting}[caption=Pseudo code for calculating the new audio feature value and looking up respective tracks with a tolerance radius from the music library, label=lst:audioFeature]
if(direction == negative) {
	newValue = cur + (cur - min) * -scale;
} else {
	newValue = cur + (max - cur) * scale;
}

while(noTracksFound) {
	lookupTracksWithValueAndTolerance(newValue, tolerance);
	increase(tolerance);
}
\end{lstlisting}

\begin{figure}[bth]
	\myfloatalign
	\includegraphics[width=.99\linewidth]{img/calcAudioFeatureExample.png}
	\caption{Example calculation for changing tracks based on the audio's tempo. The big X's show the current and the new value, while the smaller ones depict values with a smaller scaling factor. The red rectangle around the new value illustrates the tolerance radius in which new tracks are searched.}
	\label{fig:audioFeatureExample}
\end{figure}

After sending a possible speech command to the mobile application, the text needs to be converted to an applicable command. For this, an algorithm first tries to find keywords %simple group 1 and 2. complex from group 3. decision diagram from note

\subsection{Gesture Control}

%Chapter \ref{ch:implementation} 


%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************




