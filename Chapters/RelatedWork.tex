%************************************************
\chapter{Related Work}\label{ch:relatedwork}
%************************************************

Casual interaction has become a big research topic in \ac{HCI} nowadays. Since this thesis aims at using gesture and speech input it is helpful to study what other researchers have achieved in these fields.

Pohl and Murray-Smith \cite{pohl2013focused} have characterised the term casual interaction in contrast to focused interaction and described the \textit{focused-casual continuum}, which is a control-theoretic framework that characterizes input techniques in regard to how much flexibility, in terms of thinking and effort, they allow a user to invest into interactions. They showed in a user study that users adjust their level of engagement to the task's complexity.

On this basis, \cite{Busse2014Thesis} constructed a wrist worn silicone bracelet. When worn, a user could casually interact with a light source. Simple actions like turning the light on and off up to picking individual colors with a capacitive touch stripe. Accelerometer based gestures could be used to activate previously defined and memorized light settings. Despite being highly accessible on the wrist, a user would still have to utilize the hand without the bracelet to activate it's features making interactions rather impractical in certain situations.

Another approach places a depth camera for capturing hand gestures on the user's foot pointing upwards \cite{bailly2012shoesense}. This allows for discreet interactions thus neglecting concerns of social acceptability of performing gestures as they found out. In a lab study they compared physical and mental demand, user preferences and demonstrated a 94-99\% recognition rate.

An alternative input technique is shown in \cite{pohl2014around}. They introduce around-device devices. Input is received by observing position and rotation as well as arrangement or absence of the around-device devices. To capture this information they propose placing a smartphone equipped with a depth camera nearby.  In contrast to the aforementioned approaches, this technique is limited to stationary contexts automatically excluding any in-motion-situations.

Furthermore, casual interaction was applied to mobile music retrieval by \cite{boland2015engaging}. They investigated the listening habits of 95 last.fm\footnote{Internet radio station: \url{www.last.fm}} users and divided them into three groups. The first group consists of the engaged listeners who invest high initial engagement by e.g. selecting a specific album and afterwards only make quick and decisive interventions. The second group consists of the casual users who invest little effort in interventions at any time. The third group is a mixture of the first two groups where music listening behaviour highly depends on the context. Based on these groups they added a semantic zooming view of linear music space to a already given music retrieval interface. Zooming in on the view enables the user to make more specific music selections. A recommender system additionally infers other relevant music depending on the input specificity.

In the scope of interactions with smart home appliances \cite{kuhnel2011m} conducted a series of user study on gestural input for devices found in an average living room -- namely blinds, lamps, tv, \ac{EPG}, video recorder and answering machine. In the first study they tried to determine a gesture vocabulary. Therefor they observed eigtheen participants seated on a sofa in a fully functional living room with the above mentioned devices. The participants were asked to perform a gesture, they would deem appropriate, for every action or referent as \cite{wobbrock2009user} refer to. In a second study 22 new participants should then map the gestures from the vocabulary back to the referents. The last study was performed by 10 participants to study the memorability. In a training session every participant performed every gesture five times and then rated the suitability. Finally a slide show displayed every referent for 5 seconds in a random order. If the participant could not perform the gesture in this time, the correct gesture was shown again and the referent was added to the end of the slide show again. Overall their results showed, that simple and short physically or symbolic inspired gestures were rated most suitable and appeared to be most memorizable. These results were considered while designing the gestures for the music player.

In Addition, \cite{choi2012can} conducted two related user studies. 28 participants were asked to propose gestures for referents similar to the above mentioned. A month later, the same participants had to choose the most suitable gesture for each gesture group (i.e. for each referent) which also included their own derived gesture. It turned out, that 65\% of the top gestures from the first experiment were not the most chosen gestures in the second experiment. For some of the most agreed gestures in the second experiment, e.g. rubbing one's shoulders for turning off air conditioning, the frequency was only below 10\% in the first experiment. Their results show that considering only the most frequent matching user derived gestures can not automatically be considered to be most suitable.

Casual interaction through speech input is yet to be explored. Some research, however, was inquired in the field of smart homes. For example, \cite{blumendorf2008multimodal} prototyped a cooking assistant that was installed in their Ambient Living Testbed. Users could interact with the assistant either via touchscreen, mouse and keyboard or via speech input. The latter came in handy while being physically distracted as they were searching for ingredients or cutting vegetables. The findings from the user study based on the cooking assistant revealed that users prefer the availability of mutliple modalities as the possibility to fall back to touch or mouse input provides an idea safety against failures of the voice recognition. Furthermore, they state a higher acceptance of command-based speech interactions instead of entering whole sentences, as short commands are easier to learn. For more complex commands, users tend to ask the system for help.\\

The previous work in the field of casual interaction und especially the fictive scenario about Bob (see \ref{bobscenario}) show, that it is advantageous to use a device that is always accessible by wearing it around the wrist. Hence, the user can interact with the system even though her hands might be dirty or carrying a bag. Karoline's solution \cite{Busse2014Thesis} already enables interaction in a lot of possible scenarios. However, the bracelet strictly requires the other hand to not be occupied in order to perform any form of input. Even in the context of a smart home lighting system, there are situations imaginable where an interaction would not be possible. This problem reveals the need of hands-free interaction techniques. 

Using a smartwatch that supports touch, gesture and speech input over Karoline's bracelet \cite{Busse2014Thesis} has the advantage of increased computing power and battery life and it enables purely hands-free interaction. Gestures that can be performed without utilizing the second hand are required and can be used for issuing simple commands. As \cite{kuhnel2011m} found out, gestures that are short and symbolic are prefered. Speech input on the other hand can be used for more complex tasks like adding a number to a command in order to set a specific attribute of the system. It must be possible to activate the speech recognition without touching the smartwatch, though. In the end, the touch input serves as a fallback in case focused interaction is required or the other techniques can not be used or even malfunction. Together these input techniques are supposed to let the user choose the amount of engagement to invest into interactions. The viability is, however, yet to be investigated.

%Chapter \ref{ch:relatedwork} 


%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************




